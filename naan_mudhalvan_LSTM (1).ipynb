{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a11dmPw2mhfw",
        "outputId": "7468ff44-a13c-4f8b-da51-49b12b25d4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1, Loss: 0.6553\n",
            "Epoch 2, Loss: 0.6451\n",
            "Epoch 3, Loss: 0.6087\n",
            "Epoch 4, Loss: 0.7132\n",
            "Epoch 5, Loss: 0.4351\n",
            "Epoch 6, Loss: 0.2786\n",
            "Epoch 7, Loss: 0.3789\n",
            "Accuracy: 92.43%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('evaluation.csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "data2=pd.read_csv('test (1).csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "data2 = data2.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Preprocess text and labels\n",
        "texts = data['text'].astype(str)\n",
        "labels = data['label'].astype(int)\n",
        "\n",
        "texts2 = data2['text'].astype(str)\n",
        "labels2 = data2['label'].astype(int)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "labels2 = label_encoder.fit_transform(labels2)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=200, return_tensors='pt')\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze(), label\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# LSTM Model\n",
        "class FakeNewsLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FakeNewsLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, 128)\n",
        "        self.lstm = nn.LSTM(128, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.dropout(lstm_out[:, -1, :])\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "# Model instantiation\n",
        "model = FakeNewsLSTM(30522, 64, 1).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(7):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device).float()\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        predicted = (outputs > 0.5).long()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "import joblib\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the datasets\n",
        "data = pd.read_csv('train.csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "data2 = pd.read_csv('test (1).csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "data2 = data2.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Preprocess text and labels\n",
        "texts = data['text'].astype(str)\n",
        "labels = data['label'].astype(int)\n",
        "\n",
        "texts2 = data2['text'].astype(str)\n",
        "labels2 = data2['label'].astype(int)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "labels2 = label_encoder.transform(labels2)  # Use transform to ensure same encoding\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=200, return_tensors='pt')\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze(), label\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = TextDataset(texts.tolist(), labels)\n",
        "test_dataset = TextDataset(texts2.tolist(), labels2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# LSTM Model\n",
        "class FakeNewsLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FakeNewsLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, 128)\n",
        "        self.lstm = nn.LSTM(128, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.dropout(lstm_out[:, -1, :])\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "# Model instantiation\n",
        "model = FakeNewsLSTM(30522, 64, 1).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation on full test set\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        predicted = (outputs > 0.5).long()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "print(f'Final Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_9JA-e33rax",
        "outputId": "4660897a-f21e-4066-9ce0-e740d86d772b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1, Average Loss: 0.6471\n",
            "Epoch 2, Average Loss: 0.4626\n",
            "Epoch 3, Average Loss: 0.4157\n",
            "Epoch 4, Average Loss: 0.3278\n",
            "Epoch 5, Average Loss: 0.3229\n",
            "Epoch 6, Average Loss: 0.2909\n",
            "Epoch 7, Average Loss: 0.2631\n",
            "Epoch 8, Average Loss: 0.1926\n",
            "Epoch 9, Average Loss: 0.1718\n",
            "Epoch 10, Average Loss: 0.1501\n",
            "Final Test Accuracy: 93.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "import joblib\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the datasets\n",
        "data = pd.read_csv('train.csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "data2 = pd.read_csv('test (1).csv', delimiter=';', engine='python', on_bad_lines='skip').head(10000)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "data2 = data2.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Preprocess text and labels\n",
        "texts = data['text'].astype(str)\n",
        "labels = data['label'].astype(int)\n",
        "\n",
        "texts2 = data2['text'].astype(str)\n",
        "labels2 = data2['label'].astype(int)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "labels2 = label_encoder.transform(labels2)  # Use transform to ensure same encoding\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=200, return_tensors='pt')\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze(), label\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = TextDataset(texts.tolist(), labels)\n",
        "test_dataset = TextDataset(texts2.tolist(), labels2)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# LSTM Model\n",
        "class FakeNewsLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FakeNewsLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, 128)\n",
        "        self.lstm = nn.LSTM(128, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.dropout(lstm_out[:, -1, :])\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "# Model instantiation\n",
        "model = FakeNewsLSTM(30522, 64, 1).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation on full test set\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, labels in test_loader:\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        predicted = (outputs > 0.5).long()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "print(f'Final Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJWnYSdyOhjI",
        "outputId": "5f77b22f-aecf-43ff-b435-d5b4cc2ab32e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1, Average Loss: 0.6611\n",
            "Epoch 2, Average Loss: 0.5633\n",
            "Epoch 3, Average Loss: 0.6176\n",
            "Epoch 4, Average Loss: 0.5402\n",
            "Epoch 5, Average Loss: 0.6765\n",
            "Epoch 6, Average Loss: 0.4738\n",
            "Epoch 7, Average Loss: 0.2224\n",
            "Epoch 8, Average Loss: 0.1208\n",
            "Epoch 9, Average Loss: 0.1038\n",
            "Epoch 10, Average Loss: 0.1029\n",
            "Epoch 11, Average Loss: 0.0831\n",
            "Epoch 12, Average Loss: 0.0729\n",
            "Epoch 13, Average Loss: 0.0542\n",
            "Epoch 14, Average Loss: 0.0389\n",
            "Epoch 15, Average Loss: 0.0290\n",
            "Epoch 16, Average Loss: 0.0158\n",
            "Epoch 17, Average Loss: 0.0099\n",
            "Epoch 18, Average Loss: 0.0061\n",
            "Epoch 19, Average Loss: 0.0060\n",
            "Epoch 20, Average Loss: 0.0040\n",
            "Final Test Accuracy: 98.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(label_encoder, 'label_encoder2.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYPjx-OkCNUd",
        "outputId": "e66b64f2-4f60-422e-c9bb-865eaaa92826"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder2.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained('./tokenizer_dir2')\n",
        "\n",
        "print(\"All files saved successfully.\")\n",
        "from google.colab import files\n",
        "'''\n",
        "# Download the model file\n",
        "files.download('model.pth')\n",
        "\n",
        "# Download the label encoder\n",
        "files.download('label_encoder2.joblib')'''\n",
        "\n",
        "# Download the tokenizer folder as a zip\n",
        "!zip -r tokenizer_dir2.zip tokenizer_dir2\n",
        "files.download('tokenizer_dir2.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "l8rMKv6IEJeu",
        "outputId": "7ffd9fd0-8372-4a1f-e441-7a29147e3e3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files saved successfully.\n",
            "  adding: tokenizer_dir2/ (stored 0%)\n",
            "  adding: tokenizer_dir2/vocab.txt (deflated 53%)\n",
            "  adding: tokenizer_dir2/tokenizer_config.json (deflated 75%)\n",
            "  adding: tokenizer_dir2/special_tokens_map.json (deflated 42%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9d208383-05db-4e80-a296-44497a64ab4e\", \"tokenizer_dir2.zip\", 110927)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}