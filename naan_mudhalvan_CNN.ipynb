{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-eYSZg044vk",
        "outputId": "a69d515f-4508-4376-c678-b50e84bff80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.2559, Val Loss: 0.0695\n",
            "Epoch 2/10, Train Loss: 0.0577, Val Loss: 0.0635\n",
            "Epoch 3/10, Train Loss: 0.0278, Val Loss: 0.0672\n",
            "Epoch 4/10, Train Loss: 0.0105, Val Loss: 0.0768\n",
            "Early stopping triggered\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.96      0.99      0.98       878\n",
            "        Real       0.99      0.97      0.98      1122\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.98      0.98      0.98      2000\n",
            "weighted avg       0.98      0.98      0.98      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load training dataset\n",
        "df = pd.read_csv('train.csv', sep=';', on_bad_lines='skip', engine='python').head(10000)\n",
        "df['combined_text'] = df['title'] + ' ' + df['text']  # Combine title and text for input\n",
        "\n",
        "# Parameters\n",
        "max_words = 5000  # Vocabulary size (top 5000 words)\n",
        "max_len = 300     # Maximum sequence length\n",
        "embedding_dim = 100  # Embedding vector size\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Tokenize text (using Keras Tokenizer for consistency)\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['combined_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['combined_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Prepare data\n",
        "X = padded_sequences\n",
        "y = df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset class\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "test_dataset = NewsDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len, num_filters=128, kernel_size=5):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)  # Global max pooling\n",
        "        self.dense1 = nn.Linear(num_filters, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # [batch_size, max_len, embedding_dim]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, embedding_dim, max_len] for Conv1d\n",
        "        x = self.conv1d(x)  # [batch_size, num_filters, max_len - kernel_size + 1]\n",
        "        x = self.relu(x)\n",
        "        x = self.global_max_pool(x).squeeze(-1)  # [batch_size, num_filters]\n",
        "        x = self.dense1(x)  # [batch_size, 64]\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)  # [batch_size, 1]\n",
        "        x = self.sigmoid(x)  # [batch_size, 1]\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextCNN(vocab_size=max_words, embedding_dim=embedding_dim, max_len=max_len).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 2\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device).view(-1, 1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).view(-1, 1)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "        val_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.5).float().cpu().numpy()\n",
        "        y_pred.extend(preds.flatten())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Fake', 'Real']))\n",
        "\n",
        "# Save model (optional)\n",
        "torch.save(model.state_dict(), 'cnn_fake_news_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_fake_news_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EHveULYQBoIP",
        "outputId": "0aa69be1-824b-4d0a-c0dd-6cd2f0ab9d9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ccead71c-d446-4286-b9fc-d3b11e208c24\", \"cnn_fake_news_model.pth\", 2292936)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}